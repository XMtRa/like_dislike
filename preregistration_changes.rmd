---
title: "Changes From Preregistration"
output: "html_document"
---

The preregistration can be found [here](https://osf.io/a6tzc/?view_only=5d0ef9fe5e1745878cd1b19273cdf859). In what follows, please find all deviations from the preregistration.

# Theoretical Background

In general, instead of analyzing _self-disclosure_ specifically, I have decided to analyze communication behavior more broadly. Initially, I operationalized self-disclosure with the log of communication frequency plus likes and dislikes. However, based on peer-feedback, I realized that this operationalization is too coarse to represent self-disclosure. Hence, I now analyze the mere quantity of communication, which is less controversial.

## Hypotheses

 -	The order of the hypotheses was changed.
 -	The wording of the hypotheses was changed (but not the content).
 -  The enumeration of the hypotheses was changed. 
    - Before, hypotheses were enumerated from 1-13.
    - Now, it's 1, 2, 3, 4, 5a, 5b, 5c, 5d, 6a, 6b, 6c, 7a, 7b.

## Research Question

 -  I dropped two Research Questions. I realized that they just didn't make sense. That is, I dropped the RQ that popularity cues might affect self-efficacy and trust.

# Methods
## Participants
### Exclusion criteria

Initially, I planned to exclude all participants who finished the questionnaire in less than 6 minutes. However, when checking the deleted participants individually, I realized this would have led to the deletion of participants who provided answers that seemed perfectly fine. In conclusion, I relaxed the criterion to 3 minutes, which led to the exclusion of 27 participants. Results changed only marginally and do not hinge on exclusion (see additional analyses).

# Material
## Expected Benefits

In the preregistration, I stated that I would measure expected benefits using 5 general items (e.g., “Using the participation platform had many benefits for me”). However, I had also designed additional items for more specific gratifications that I did not include in the preregistration (the preregistration manual stated that additional variables that one does not plan to analyze do not need to be preregistered). These specific measures of gratifications were hence used for exploratory analyses.

## Trust

Originally, I operationalized trust for three entities (i.e., provider, website, and other users) using four subdimensions (i.e., general trust, ability, benevolence, and integrity). Only later did I realize that the literature differentiates between general and specific trust beliefs, which I could conceptualize with the items I measured. As a result, in the paper I now differentiate both dimensions.

# Results

- Given that I found not apparent effects of the three websites on the privacy calculus, I did not test for indirect effects as proposed in the preregistration. 
- I additionally also controlled for education (I forgot to explicitly mention this control variable in the preregistration; results do not differ, but I think it should be included, which is why I did).

# Power analyses

In the preregistration, power analyses were conducted assuming 80% statistical power. However, in the meantime I have come to believe that in most cases and if logistically possible it's better to strive for balanced alpha and beta errors, which given an alpha of 5% leads to a desired power of 95%. As a result, report power analyses aimed for 95% power.

# Inferences

In the preregistration, it was stated that I would analyze the additional exploratory analyses using Bonferroni-Holm correction. However, in the meantime I have come to understand that formal inferences tests for exploratory analyses are debatable, for example because the number of tests one could potentially account for is infinite. Instead, I decided not to make strong inferences on the basis of the exploratory analyses.